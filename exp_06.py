# -*- coding: utf-8 -*-
"""Exp_06

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-X9SdSYoPmRReb6nYKpNh-_zbVoJIRMN
"""

import pandas as pd
import numpy as np

data = pd.read_csv('/content/adult.csv')
print(data)

data = data.replace(["?"], np.nan)
print(data.isnull().sum())
data = data.drop(["fnlwgt","education"], axis = 1)

categorical_columns = ["workclass", "occupation", "native-country"]

for column in categorical_columns:
    mode_value = data[column].mode()[0]  # Calculate the mode
    data[column].fillna(mode_value, inplace=True)  # Fill missing values with mode

# Check again for missing values to ensure they are filled
print(data.isnull().sum())

colname = []

for i in data.columns:
    if(data[i].dtype == "object"):
        colname.append(i)

colname

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for i in colname:
    data[i] = le.fit_transform(data[i])
print(data.head())

X = data.values[:, :-1]
Y = data.values[:, -1]

print(X)
print(Y)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Assuming X and Y are already defined from your previous steps
# Split the dataset into training and testing sets (70% train, 30% test)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=10)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)  # Fit and transform on training data
X_test = scaler.transform(X_test)        # Only transform on testing data

# Apply PCA without reducing dimensions (to analyze variance)
pca = PCA(n_components=None)
X_train_pca_full = pca.fit_transform(X_train)  # Fit PCA and transform training data
X_test_pca_full = pca.transform(X_test)        # Transform testing data

# Print explained variance ratio for each principal component
explained_variance_full = pca.explained_variance_ratio_
print("\nExplained Variance (All Components):", explained_variance_full)

# Now apply PCA to retain 75% of the variance
pca = PCA(n_components=0.75)
X_train = pca.fit_transform(X_train)  # Fit PCA and transform training data
X_test = pca.transform(X_test)        # Transform testing data

# Print explained variance ratio for the selected components
explained_variance_reduced = pca.explained_variance_ratio_
print("\nExplained Variance (75% Components):", explained_variance_reduced)

# Number of components selected to explain 75% variance
n_components_selected = pca.n_components_
print("\nNumber of Components Selected:", n_components_selected)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

print("Confusion Matrix = ")
print(confusion_matrix(Y_test, Y_pred), "\n")
print("Accuracy Score = ", accuracy_score(Y_test, Y_pred), "\n")

pca = PCA(n_components=0.8)
X_train = pca.fit_transform(X_train)  # Fit PCA and transform training data
X_test = pca.transform(X_test)
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

print("Confusion Matrix = ")
print(confusion_matrix(Y_test, Y_pred), "\n")
print("Accuracy Score = ", accuracy_score(Y_test, Y_pred), "\n")